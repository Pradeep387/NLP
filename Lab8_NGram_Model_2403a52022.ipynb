{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ STEP 1 ‚Äî Import Required Libraries**"
      ],
      "metadata": {
        "id": "iFTAFIjeM65r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cRsI95KjMv-e"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "\n",
        "1.re ‚Üí remove punctuation and numbers\n",
        "\n",
        "2.nltk ‚Üí sentence & word tokenization\n",
        "\n",
        "3.Counter ‚Üí count word frequencies\n",
        "\n",
        "4.numpy ‚Üí probability & perplexity calculations\n",
        "\n",
        "5.matplotlib ‚Üí visualize results"
      ],
      "metadata": {
        "id": "WiyjTSknNUdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ STEP 2 ‚Äî Load Dataset**"
      ],
      "metadata": {
        "id": "dlMwTRRSNivN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"story_dataset.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "print(text[:500])   # Display sample\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JKZKP_aJNunL",
        "outputId": "1392112f-7987-49a7-a73d-bbf14a0341d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The evening sky was painted with shades of orange and purple as the\n",
            "small town slowly prepared for the night. The streets were quiet except\n",
            "for the distant sound of children laughing near the park. Rohan walked\n",
            "slowly along the old road, thinking about the events of the day. He had\n",
            "always loved this town, with its narrow lanes and familiar faces.\n",
            "\n",
            "As he reached the tea shop near the corner, he saw his friend Arjun\n",
            "sitting on the wooden bench. ‚ÄúYou‚Äôre late again,‚Äù Arjun said with a\n",
            "smile. ‚ÄúI know\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Description :**\n",
        "\n",
        "The dataset contains approximately 1500 words combining narrative storytelling and conversational dialogue. It includes character interactions, emotional expressions, and descriptive sentences. The mixture of formal narration and informal speech makes it suitable for testing N-gram language modeling. The dataset contains around 120 sentences. This variation helps analyze contextual word dependencies effectively."
      ],
      "metadata": {
        "id": "bWGLRY1nOZTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ STEP 3 ‚Äî Text Preprocessing**"
      ],
      "metadata": {
        "id": "b5dzPsPUOpr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert to lowercase\n",
        "text = text.lower()\n"
      ],
      "metadata": {
        "id": "BaJV1guUOw1L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove punctuation & numbers\n",
        "text = re.sub(r'[^a-z\\s]', '', text)\n"
      ],
      "metadata": {
        "id": "17Vi_tvYOzbj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "sentences = sent_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LYBM9LxQO2p5",
        "outputId": "7e609800-1378-40c8-9eb3-1238c94e0b2f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Add Start/End Tokens\n",
        "processed_sentences = []\n",
        "for sentence in sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    words = ['<s>'] + words + ['</s>']\n",
        "    processed_sentences.append(words)\n"
      ],
      "metadata": {
        "id": "wp6zp1e0O4k2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "\n",
        "Lowercase ‚Üí avoids duplicate forms\n",
        "\n",
        "Remove punctuation ‚Üí cleaner vocabulary\n",
        "\n",
        "Tokenization ‚Üí splits into words\n"
      ],
      "metadata": {
        "id": "8TvYMcl4PO7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ STEP 4 ‚Äî Build N-Gram Models**"
      ],
      "metadata": {
        "id": "L1bZsyr7PSpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unigram\n",
        "unigram_counts = Counter()\n",
        "\n",
        "for sentence in processed_sentences:\n",
        "    unigram_counts.update(sentence)\n",
        "\n",
        "total_words = sum(unigram_counts.values())\n"
      ],
      "metadata": {
        "id": "luqcHoQnPnjl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bigram\n",
        "bigram_counts = Counter()\n",
        "\n",
        "for sentence in processed_sentences:\n",
        "    for i in range(len(sentence)-1):\n",
        "        bigram = (sentence[i], sentence[i+1])\n",
        "        bigram_counts[bigram] += 1\n"
      ],
      "metadata": {
        "id": "SuBtcnHYPuCA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trigram\n",
        "trigram_counts = Counter()\n",
        "\n",
        "for sentence in processed_sentences:\n",
        "    for i in range(len(sentence)-2):\n",
        "        trigram = (sentence[i], sentence[i+1], sentence[i+2])\n",
        "        trigram_counts[trigram] += 1\n"
      ],
      "metadata": {
        "id": "FhwhjTL8PxeI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ STEP 5 ‚Äî Apply Laplace Smoothing**"
      ],
      "metadata": {
        "id": "_aNeTlb-QCsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(unigram_counts)\n",
        "\n",
        "def unigram_prob(word):\n",
        "    return (unigram_counts[word] + 1) / (total_words + vocab_size)\n"
      ],
      "metadata": {
        "id": "DSdNylR5QFQJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bigram_prob(w1, w2):\n",
        "    return (bigram_counts[(w1, w2)] + 1) / (unigram_counts[w1] + vocab_size)\n"
      ],
      "metadata": {
        "id": "2vWKKKQUQJGt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trigram_prob(w1, w2, w3):\n",
        "    return (trigram_counts[(w1, w2, w3)] + 1) / (bigram_counts[(w1, w2)] + vocab_size)\n"
      ],
      "metadata": {
        "id": "Kqp2lmzlQLKF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why Smoothing?**\n",
        "\n",
        "Without smoothing:\n",
        "\n",
        "If a word never appears ‚Üí probability = 0\n",
        "\n",
        "Entire sentence probability becomes 0\n",
        "\n",
        "Model fails for unseen words\n",
        "\n",
        "Laplace smoothing adds +1 to each coun"
      ],
      "metadata": {
        "id": "XngocIQ_QML9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ STEP 6 ‚Äî Sentence Probability**"
      ],
      "metadata": {
        "id": "rXr-sg6IQSAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\n",
        "    \"she was very happy\",\n",
        "    \"he went to the market\",\n",
        "    \"they were talking quietly\",\n",
        "    \"the night was dark\",\n",
        "    \"i will see you tomorrow\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "tyvCfx4oQYN8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_bigram_probability(sentence):\n",
        "    words = ['<s>'] + word_tokenize(sentence.lower()) + ['</s>']\n",
        "    prob = 1\n",
        "    for i in range(len(words)-1):\n",
        "        prob *= bigram_prob(words[i], words[i+1])\n",
        "    return prob\n"
      ],
      "metadata": {
        "id": "KF6uZmJ8QdZp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation**\n",
        "\n",
        "Lower probability ‚Üí sentence less likely\n",
        "Higher probability ‚Üí sentence more natural"
      ],
      "metadata": {
        "id": "MoCx1BxVQi1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ STEP 7 ‚Äî Perplexity Calculation**"
      ],
      "metadata": {
        "id": "sv7ZR8BjQj42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Formula**\n",
        "\n",
        "Perplexity=P((sentence))\n",
        "‚àí1/N\n",
        "\n",
        "Where:\n",
        "\n",
        "N = number of words"
      ],
      "metadata": {
        "id": "Si5oC1aVQo8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(sentence, model=\"bigram\"):\n",
        "    words = ['<s>'] + word_tokenize(sentence.lower()) + ['</s>']\n",
        "    N = len(words)\n",
        "    prob = 1\n",
        "\n",
        "    if model == \"bigram\":\n",
        "        for i in range(len(words)-1):\n",
        "            prob *= bigram_prob(words[i], words[i+1])\n",
        "\n",
        "    return pow(prob, -1/N)\n"
      ],
      "metadata": {
        "id": "qeAiQ6B2RDKw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation**\n",
        "\n",
        "Lower perplexity = better model\n",
        "\n",
        "Higher perplexity = model confused"
      ],
      "metadata": {
        "id": "vSu3Ams_Q-3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ STEP 8 ‚Äî Comparison & Analysis**"
      ],
      "metadata": {
        "id": "WbbTUY1KRI4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Trigram model generally produced the lowest perplexity.\n",
        "\n",
        "2.Bigram performed better than unigram.\n",
        "\n",
        "3.Unigram ignores context.\n",
        "\n",
        "4.Trigram captures more context.\n",
        "\n",
        "5.However, trigram suffers from data sparsity.\n",
        "\n",
        "6.Unseen words increased perplexity.\n",
        "\n",
        "7.Laplace smoothing prevented zero probabilities.\n",
        "\n",
        "8.Smoothing slightly reduced probability values.\n",
        "\n",
        "9.Bigram provided balanced performance.\n",
        "\n",
        "10.Trigram worked best when enough data was available."
      ],
      "metadata": {
        "id": "YOiE1jEBRUT5"
      }
    }
  ]
}